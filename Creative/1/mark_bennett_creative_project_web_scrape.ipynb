{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "mark_bennett_creative_project_web_scrape.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "T0_Aabi-uwBX"
      },
      "source": [
        "##https://towardsdatascience.com/how-to-web-scrape-with-python-in-4-minutes-bc49186a8460\r\n",
        "\r\n",
        "import requests\r\n",
        "import urllib.request\r\n",
        "import time\r\n",
        "import array\r\n",
        "import textwrap\r\n",
        "from bs4 import BeautifulSoup"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZGzRAI3gu7uu",
        "outputId": "0902f35b-3e3f-4a30-8d83-a2a7c5e93378"
      },
      "source": [
        "#website used\r\n",
        "url = 'http://obamaspeeches.com/'\r\n",
        "response = requests.get(url)\r\n",
        "print(response)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<Response [200]>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f1xxpXs2vgDw"
      },
      "source": [
        "# gets the webpage text\r\n",
        "soup = BeautifulSoup(response.text, \"html.parser\")"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vg6LYHkWwU8V"
      },
      "source": [
        "##loops through all the links on the site\r\n",
        "## removes outside links\r\n",
        "## remomves being slash on links\r\n",
        "## appends website base url and puts it into a list\r\n",
        "linklist = []\r\n",
        "\r\n",
        "for link in soup.find_all('a'):\r\n",
        "  x = link.get('href')\r\n",
        "  if x.startswith('http'):\r\n",
        "    continue\r\n",
        "  elif x.startswith('/'):\r\n",
        "    x = x[1:]\r\n",
        "    linklist.append(url + x)\r\n",
        "  else:\r\n",
        "    linklist.append(url + x)\r\n"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "niAc20MVOHgV"
      },
      "source": [
        "# using list comprehension \r\n",
        "# to remove duplicated  \r\n",
        "# from list  \r\n",
        "#https://www.geeksforgeeks.org/python-ways-to-remove-duplicates-from-list/\r\n",
        "res = [i for n, i in enumerate(linklist) if i not in linklist[:n]] "
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qo3oCuPnPDoe"
      },
      "source": [
        "print(res)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F7gL-8qZPbb9"
      },
      "source": [
        "# Search all the pages off the website \r\n",
        "#take the content of each page and combine them\r\n",
        "speech =\"\"\r\n",
        "for url2  in res:\r\n",
        "    page = requests.get(url2)\r\n",
        "    soup2 = BeautifulSoup(page.text, \"html.parser\")\r\n",
        "\r\n",
        "    test = soup2.find(\"table\", attrs={\"width\": \"610\"})\r\n",
        "\r\n",
        "    if test:\r\n",
        "        s = test.get_text()\r\n",
        "        speech += s\r\n"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kwkhiXvqf8hU"
      },
      "source": [
        "print(speech)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V3ENJeAggOPF"
      },
      "source": [
        "#save everything to a text file\r\n",
        "file = open(\"obamaspeech.txt\", \"w\") \r\n",
        "file.write(speech) \r\n",
        "file.close() "
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}